[INFO] main.py:55 > Set the device (cuda)
[INFO] augment.py:18 > cifar10: autoaugmentation is applied
[INFO] main.py:85 > Using train-transforms Compose(
    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=None)
    RandomCrop(size=(32, 32), padding=4)
    RandomHorizontalFlip(p=0.5)
    AutoAugment CIFAR10 Policy
    ToTensor()
    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))
)
[INFO] main.py:95 > [1] Select a CIL method (rm)
[INFO] method_manager.py:97 > CIL Scenario: 
[INFO] main.py:101 > [2] Incrementally training 5 tasks
[INFO] main.py:112 > [2-1] Prepare a datalist for the current task
[INFO] data_loader.py:76 > [Train] Get datalist from cifar10_train_blurry10_rand1_cls2_task0.json
[INFO] data_loader.py:109 > [Test ] Get datalist from cifar10_test_rand1_cls2_task0.json
[INFO] data_loader.py:109 > [Test ] Get datalist from cifar10_test_rand1_cls2_task1.json
[INFO] data_loader.py:109 > [Test ] Get datalist from cifar10_test_rand1_cls2_task2.json
[INFO] data_loader.py:109 > [Test ] Get datalist from cifar10_test_rand1_cls2_task3.json
[INFO] data_loader.py:109 > [Test ] Get datalist from cifar10_test_rand1_cls2_task4.json
[INFO] main.py:129 > [2-2] Set environment for the current task
[INFO] finetune.py:98 > Apply before_task
[INFO] finetune.py:129 > Reset the optimizer and scheduler states
[INFO] finetune.py:134 > Increasing the head of fc 10 -> 10
[INFO] main.py:135 > [2-3] Start to train under online
[INFO] main.py:152 > Train over streamed data once
[INFO] rainbow_memory.py:70 > Streamed samples: 9750
[INFO] rainbow_memory.py:71 > In-memory samples: 0
[INFO] rainbow_memory.py:72 > Train samples: 9750
[INFO] rainbow_memory.py:73 > Test samples: 10000
[INFO] rainbow_memory.py:103 > Task 0 | Epoch 1/1 | train_loss 1.2035 | train_acc 0.5092 | test_loss 3.0482 | test_acc 0.1828 | lr 0.0050
[INFO] finetune.py:155 > Update memory over 10 classes by 
[ERROR] finetune.py:183 > Not implemented memory management
